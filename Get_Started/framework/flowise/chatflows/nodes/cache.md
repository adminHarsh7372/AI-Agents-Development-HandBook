# ðŸ—‚ï¸ Cache Node

The `Cache` node in Flowise helps you **store and reuse previous LLM outputs** to reduce cost, improve speed, and avoid repeated computation.

Flowise uses Langchain's caching system under the hood â€” especially useful when dealing with repeated inputs or heavy prompts.

---

## âš¡ Why Use Cache?

| Benefit            | What It Means                                                  |
|--------------------|----------------------------------------------------------------|
| ðŸ§  Smarter Performance | Avoid re-querying the LLM for the same input                  |
| ðŸ’¸ Cost Efficient   | Saves tokens on repeated calls (especially with OpenAI/Groq)   |
| âš¡ Faster Responses | Zero waiting when inputs are already seen                      |
| ðŸ“Š Useful in Loops  | Reduces latency in iterative or agent-style workflows          |

---

## ðŸ› ï¸ How the Cache Works

1. User input is received
2. Flowise checks if **that exact input** exists in the cache
3. If yes â†’ returns cached response
4. If no â†’ sends input to LLM â†’ saves result in cache

---

## ðŸ§± Cache Node Inputs & Outputs

| Input           | Description                          |
|------------------|--------------------------------------|
| Input Text       | The original prompt or query         |
| LLM              | Optional, used to trigger fallback   |

| Output           | Description                          |
|------------------|--------------------------------------|
| Cached Response  | Text returned from memory or LLM     |

---

## ðŸ”§ Configuration Options

| Field              | Description                                     |
|--------------------|-------------------------------------------------|
| Cache Backend      | `InMemory` or `Redis` *(if supported)*          |
| Namespace          | (Optional) Custom label to organize cache       |
| Auto Save          | Whether to store every result automatically     |
| Manual Control     | You can also trigger saving using logic nodes   |

---

## âš™ï¸ Example Use Case

> **Prompt Caching for Smart Reply Bot**

```text
[Chat Input] â†’ [Cache Node] â†’ [LLM (fallback)] â†’ [Text Output]
``` 
If the same user query is sent twice, Flowise will return the cached result instantly.

#### ðŸ§ª Tips for Using Cache Effectively
- Use caching in loops, debug mode, or when testing prompts
- Combine with Prompt Template to make sure formatting is consistent
- If using Redis (advanced), cache becomes persistent across sessions

---
#### âœ… Summary
- The Cache node improves performance & saves tokens
- Especially useful in production agents with repeated prompts
- Works silently in the background, but can be customized

```text
Use cache when:
âœ… Testing LLM flows
âœ… Reducing cost
âœ… Improving agent response time
```