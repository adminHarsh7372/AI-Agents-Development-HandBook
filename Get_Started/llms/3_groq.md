# âš¡ Using Groq in AI Agents

**Groq** isnâ€™t just another LLM â€” itâ€™s an ultra-fast LLM **execution engine** built to run models like LLaMA and Mixtral at crazy speeds (100+ tokens/sec). Itâ€™s perfect when **latency matters** â€” like in real-time agents, chat apps, or rapid pipelines.

If you want blazing speed with decent quality â€” Groq is worth exploring.

---

## ğŸš€ Why Use Groq?

- âš¡ Insanely fast output (up to 500+ tokens/sec)
- ğŸ’µ Free to use (as of now)
- ğŸ§  Runs top open-source models like Mixtral and LLaMA 3
- ğŸŒ Easy API access, no GPU needed on your end

---

## ğŸ§  What Models Run on Groq?

| Model Name       | Type        | Strengths                         |
|------------------|-------------|-----------------------------------|
| `Mixtral-8x7B`   | Mixture of Experts | Great speed + accuracy balance     |
| `LLaMA3-8B`      | Meta's model | Lightweight, solid reasoning       |
| `Gemma-7B`       | Google       | Clean code output, multilingual    |

> Groq doesnâ€™t build its own LLM â€” it **executes existing open-source models faster.**

---

## ğŸ› ï¸ Getting Started with Groq

1. Go to [https://console.groq.com/](https://console.groq.com/)
2. Sign in with GitHub or Google
3. Create an API key
4. Add it to your `.env` file: 
**GROQ_API_KEY=your-key-here**
---

## âš™ï¸ Example (Python + Requests)

```python
import requests

response = requests.post(
 "https://api.groq.com/openai/v1/chat/completions",
 headers={
     "Authorization": "Bearer YOUR_GROQ_API_KEY",
     "Content-Type": "application/json"
 },
 json={
     "model": "mixtral-8x7b-32768",
     "messages": [
         {"role": "user", "content": "Summarize this email..."}
     ]
 }
)

print(response.json()["choices"][0]["message"]["content"])
```

### ğŸ§© Groq in LangChain
```python 
from langchain.chat_models import ChatGroq

llm = ChatGroq(
    model_name="mixtral-8x7b-32768",
    temperature=0.2
)

response = llm.invoke("Write a one-line smart reply.")
print(response)
```
### ğŸ“Œ When to Use Groq in Agents
âœ… Use Groq when:

- You want ultra-fast response agents
- You're okay with slightly less reasoning power than GPT-4
- Youâ€™re building real-time bots, chat tools, or embedded agents

### âŒ Avoid if:

- You need tool-calling, code execution, or fine-tuned reliability
- You want tight integration with Gmail, Drive, or paid LLM tools

### âœ… Summary
Groq brings speed to the LLM game â€” ideal for fast, lightweight AI agents. Pair it with open-source models for instant, responsive assistants that wonâ€™t burn your budget.

**For fast chatbots, inbox agents, or front-end helpers â€” Groq is ğŸ”¥.**

