# üíæ Using Ollama (Local LLMs) in AI Agents

**Ollama** lets you run powerful open‚Äësource LLMs entirely **locally**, without relying on cloud APIs. It's great for privacy, offline work, or cost-sensitive setups.

---

## üß† Why Use Ollama?

- üîí Full privacy‚Äîyour data never leaves your machine
- üí∞ No per‚Äëtoken API costs
- ‚öôÔ∏è Easy local server with a simple REST API
- üìö Supports many open‚Äësource models (LLaMA, LLaVA, Phi‚Äë4, etc.).

---

## üõ†Ô∏è Setup & Installation

1. **Install Ollama CLI/app**  
   - macOS: `brew install ollama`  
   - Linux/WSL: `curl -fsSL https://ollama.com/install.sh` 

2. **Pull a model**, e.g.:  
   ```bash
   ollama pull llama3.2
    ```
3. **Run the model locally**
    ```bash
    ollama run llama3.2
    ```
4. **Serve REST API:**
    ```bash
    ollama serve
    ```
    ‚Üí Available at http://127.0.0.1:11434

### üîß Example: REST API & Python
***Using cURL:**
```bash
   curl http://localhost:11434/api/generate \ 
   -d '{ "model": "llama3.2", "prompt": "What is Ollama?" }'
  ```

```python
import requests
resp = requests.post(
  "http://127.0.0.1:11434/api/generate",
  json={"model": "llama3.2", "prompt": "Explain Ollama LLM"}
)
print(resp.json()["completion"])
``` 
---

## ‚öôÔ∏è Using Ollama with LangChain or LlamaIndex

With **LangChain**:
```python
from langchain_ollama.llms import OllamaLLM
model = OllamaLLM(model="llama3.1", request_timeout=120.0)
resp = model.complete("Hello Ollama!")
print(resp)
``` 

With **LlamaIndex**:
```python
from llama_index.llms.ollama import Ollama
llm = Ollama(model="llama3.1:latest", context_window=8000)
print(llm.complete("Who is Alan Turing?"))
``` 

---

## üß© Pros & Cons

| ‚úÖ Pros                                          | ‚ùå Cons                                       |
|--------------------------------------------------|-----------------------------------------------|
| Privacy-first & offline                        | Needs local compute resources (RAM/CPU/GPU)  |
| No token/API costs                              | Performance varies by model/hardware         |
| Control over model choice & updates             | Lacks native tool-calling support            |
| Suitable for multimodal & RAG usage            | Requires local setup and maintenance         |

---

## ‚úÖ Use Cases

Use Ollama when you need:
- Offline demos or workshops
- Cost-effective model hosting
- On-device assistants with full data control
- Integrations with LlamaIndex, local AI apps

---

## üìå Summary

Ollama is a powerful local-first alternative to cloud LLMs. It empowers developers to:
- Pull and run open-source models like LLaMA and Phi‚Äë4
- Serve them via API on localhost
- Integrate with LangChain or LlamaIndex for agent workflows

